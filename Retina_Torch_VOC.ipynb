{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import Any, Callable, Dict, Optional, Tuple, List\n",
    "import warnings\n",
    "import tarfile\n",
    "import collections\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "from torch import optim\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import os\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cpath=os.getcwd()\n",
    "path2data = cpath+'\\content\\Voc'\n",
    "if not os.path.exists(path2data):\n",
    "    os.mkdir(path2data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"aeroplane\",\n",
    "    \"bicycle\",\n",
    "    \"bird\",\n",
    "    \"boat\",\n",
    "    \"bottle\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"cat\",\n",
    "    \"chair\",\n",
    "    \"cow\",\n",
    "    \"diningtable\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"motorbike\",\n",
    "    \"person\",\n",
    "    \"pottedplant\",\n",
    "    \"sheep\",\n",
    "    \"sofa\",\n",
    "    \"train\",\n",
    "    \"tvmonitor\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch에서 제공하는 VOC dataset을 상속받아, custom dataset을 생성합니다.\n",
    "class myVOCDetection(VOCDetection):\n",
    "    def __getitem__(self, index):\n",
    "        img = np.array(Image.open(self.images[index]).convert('RGB'))\n",
    "        target = self.parse_voc_xml(ET.parse(self.annotations[index]).getroot()) # xml파일 분석하여 dict으로 받아오기\n",
    "\n",
    "        targets = [] # 바운딩 박스 좌표\n",
    "        labels = [] # 바운딩 박스 클래스\n",
    "\n",
    "        # 바운딩 박스 정보 받아오기\n",
    "        for t in target['annotation']['object']:\n",
    "            label = np.zeros(5)\n",
    "            label[:] = t['bndbox']['xmin'], t['bndbox']['ymin'], t['bndbox']['xmax'], t['bndbox']['ymax'], classes.index(t['name'])\n",
    "\n",
    "            targets.append(list(label[:4])) # 바운딩 박스 좌표\n",
    "            labels.append(label[4])         # 바운딩 박스 클래스\n",
    "\n",
    "        if self.transforms:\n",
    "            augmentations = self.transforms(image=img, bboxes=targets)\n",
    "            img = augmentations['image']\n",
    "            targets = augmentations['bboxes']\n",
    "\n",
    "        return img, targets, labels\n",
    "\n",
    "    def parse_voc_xml(self, node: ET.Element) -> Dict[str, Any]: # xml 파일을 dictionary로 반환\n",
    "        voc_dict: Dict[str, Any] = {}\n",
    "        children = list(node)\n",
    "        if children:\n",
    "            def_dic: Dict[str, Any] = collections.defaultdict(list)\n",
    "            for dc in map(self.parse_voc_xml, children):\n",
    "                for ind, v in dc.items():\n",
    "                    def_dic[ind].append(v)\n",
    "            if node.tag == \"annotation\":\n",
    "                def_dic[\"object\"] = [def_dic[\"object\"]]\n",
    "            voc_dict = {node.tag: {ind: v[0] if len(v) == 1 else v for ind, v in def_dic.items()}}\n",
    "        if node.text:\n",
    "            text = node.text.strip()\n",
    "            if not children:\n",
    "                voc_dict[node.tag] = text\n",
    "        return voc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, validation dataset을 생성합니다.\n",
    "train_ds = myVOCDetection(path2data, year='2007', image_set='train', download=False)\n",
    "val_ds = myVOCDetection(path2data, year='2007', image_set='val', download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 이미지 확인\n",
    "img, target, label = train_ds[2]\n",
    "colors = np.random.randint(0, 255, size=(80,3), dtype='uint8') # 바운딩 박스 색상\n",
    "\n",
    "# 시각화 함수\n",
    "def show(img, targets, labels, classes=classes):\n",
    "    img = to_pil_image(img)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    targets = np.array(targets)\n",
    "    W, H = img.size\n",
    "\n",
    "    for tg,label in zip(targets,labels):\n",
    "        id_ = int(label) # class\n",
    "        bbox = tg[:4]    # [x1, y1, x2, y2]\n",
    "\n",
    "        color = [int(c) for c in colors[id_]]\n",
    "        name = classes[id_]\n",
    "\n",
    "        draw.rectangle(((bbox[0], bbox[1]), (bbox[2], bbox[3])), outline=tuple(color), width=3)\n",
    "        draw.text((bbox[0], bbox[1]), name, fill=(255,255,255,0))\n",
    "    plt.imshow(np.array(img))\n",
    "\n",
    "#plt.figure(figsize=(10,10))\n",
    "#show(img, target, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms 정의\n",
    "IMAGE_SIZE = 600\n",
    "scale = 1.0\n",
    "\n",
    "# 이미지에 padding을 적용하여 종횡비를 유지시키면서 크기가 600x600 되도록 resize 합니다.\n",
    "train_transforms = A.Compose([\n",
    "                    A.LongestMaxSize(max_size=int(IMAGE_SIZE * scale)),\n",
    "                    A.PadIfNeeded(min_height=int(IMAGE_SIZE*scale), min_width=int(IMAGE_SIZE*scale),border_mode=cv2.BORDER_CONSTANT),\n",
    "                    ToTensorV2()\n",
    "                    ],\n",
    "                    bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.4, label_fields=[])\n",
    "                    )\n",
    "\n",
    "val_transforms = A.Compose([\n",
    "                    A.LongestMaxSize(max_size=int(IMAGE_SIZE * scale)),\n",
    "                    A.PadIfNeeded(min_height=int(IMAGE_SIZE*scale), min_width=int(IMAGE_SIZE*scale),border_mode=cv2.BORDER_CONSTANT),\n",
    "                    ToTensorV2()\n",
    "                    ],\n",
    "                    bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.4, label_fields=[])\n",
    "                    )\n",
    "\n",
    "# transforms 적용하기\n",
    "train_ds.transforms = train_transforms\n",
    "val_ds.transforms = val_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataEncoder:\n",
    "    def __init__(self):\n",
    "        self.anchor_areas = [32*32., 64*64., 128*128., 256*256., 512*512.] # 피쳐맵 크기 p3 -> p7\n",
    "        self.aspect_ratios = [1/2., 1/1., 2/1.]            # 앵커 박스 종횡비, w/h\n",
    "        self.scale_ratios = [1., pow(2,1/3.), pow(2,2/3.)] # 앵커 박스 scale\n",
    "        self.anchor_wh = self._get_anchor_wh() # 5개의 피쳐맵 각각에 해당하는 9개의 앵커 박스 생성 \n",
    "\n",
    "    def _get_anchor_wh(self):\n",
    "        # 각 피쳐맵에서 사용할 앵커 박스 높이와 넓이를 계산합니다.\n",
    "        anchor_wh = []\n",
    "        for s in self.anchor_areas: # 각 피쳐맵 크기 추출\n",
    "            for ar in self.aspect_ratios: # ar = w/h\n",
    "                h = math.sqrt(s/ar)\n",
    "                w = ar * h\n",
    "                for sr in self.scale_ratios: # scale\n",
    "                    anchor_h = h*sr\n",
    "                    anchor_w = w*sr\n",
    "                    anchor_wh.append([anchor_w, anchor_h])\n",
    "        num_fms = len(self.anchor_areas)\n",
    "        return torch.Tensor(anchor_wh).view(num_fms, -1, 2) # [#fms, #anchors_pre_cell, 2], [5, 9, 2]\n",
    "\n",
    "    def _get_anchor_boxes(self, input_size):\n",
    "        # 피쳐맵의 모든 cell에 앵커 박스 할당\n",
    "        num_fms = len(self.anchor_areas) # 5\n",
    "        fm_sizes = [(input_size/pow(2.,i+3)).ceil() for i in range(num_fms)] # 각 피쳐맵 stride 만큼 입력 크기 축소\n",
    "\n",
    "        boxes = []\n",
    "        for i in range(num_fms): # p3 ~ p7\n",
    "            fm_size = fm_sizes[i] # i 번째 피쳐맵 크기 추출\n",
    "            grid_size = input_size / fm_size # 입력 크기를 피쳐맵 크기로 나누어 grid size 생성\n",
    "            fm_w, fm_h = int(fm_size[0]), int(fm_size[1])\n",
    "            xy = self._meshgrid(fm_w, fm_h) + 0.5 #[fm_h * fm_w, 2] 피쳐맵 cell index 생성\n",
    "            #print(xy.dtype)\n",
    "            xy = xy.to(torch.float)#torch.FloatTensor(xy)\n",
    "            #xy = torch.cuda.FloatTensor(xy)\n",
    "            \n",
    "            xy = (xy*grid_size).view(fm_h, fm_w, 1, 2).expand(fm_h,fm_w,9,2) # anchor 박스 좌표\n",
    "            wh = self.anchor_wh[i].view(1,1,9,2).expand(fm_h, fm_w, 9, 2) # anchor 박스 높이와 너비\n",
    "            box = torch.cat([xy,wh],3) # [x,y,w,h]\n",
    "            boxes.append(box.view(-1,4))\n",
    "        return torch.cat(boxes, 0)\n",
    "\n",
    "    # 피쳐맵의 각 셀에 anchor 박스 생성하고, positive와 negative 할당\n",
    "    def encode(self, boxes, labels, input_size):\n",
    "        input_size = torch.Tensor([input_size, input_size]) if isinstance(input_size, int) else torch.Tensor(input_size)\n",
    "        anchor_boxes = self._get_anchor_boxes(input_size) # 앵커 박스 생성\n",
    "        boxes = self._change_box_order(boxes, 'xyxy2xywh') # xyxy -> cxcywh\n",
    "\n",
    "        ious = self._box_iou(anchor_boxes, boxes, order='xywh') # ground-truth와 anchor의 iou 계산\n",
    "        max_ious, max_ids = ious.max(1) # 가장 높은 iou를 지닌 앵커 추출\n",
    "        boxes = boxes[max_ids]\n",
    "\n",
    "        # 앵커 박스와의 offset 계산\n",
    "        loc_xy = (boxes[:,:2]-anchor_boxes[:,:2]) / anchor_boxes[:,2:]\n",
    "        loc_wh = torch.log(boxes[:,2:]/anchor_boxes[:,2:])\n",
    "        loc_targets = torch.cat([loc_xy, loc_wh], 1)\n",
    "\n",
    "        # class 할당\n",
    "        cls_targets = 1 + labels[max_ids]\n",
    "        cls_targets[max_ious<0.5] = 0 # iou < 0.5 anchor는 negative\n",
    "        ignore = (max_ious>0.4) & (max_ious<0.5) # [0.4,0.5] 는 무시\n",
    "        cls_targets[ignore] = -1\n",
    "        return loc_targets, cls_targets\n",
    "\n",
    "    # encode된 값을 원래대로 복구 및 nms 진행\n",
    "    def decode(self,loc_preds, cls_preds, input_size):\n",
    "        cls_thresh = 0.5\n",
    "        nms_thresh = 0.5\n",
    "\n",
    "        input_size = torch.Tensor([input_size,input_size]) if isinstance(input_size, int) else torch.Tensor(input_size)\n",
    "        anchor_boxes = self._get_anchor_boxes(input_size) # 앵커 박스 생성\n",
    "\n",
    "        loc_xy = loc_preds[:,:2] # 결과값 offset 추출\n",
    "        loc_wh = loc_preds[:,2:]\n",
    "\n",
    "        xy = loc_xy * anchor_boxes[:,2:] + anchor_boxes[:,:2] # offset + anchor\n",
    "        wh = loc_wh.exp() * anchor_boxes[:,2:]\n",
    "        boxes = torch.cat([xy-wh/2, xy+wh/2], 1)\n",
    "\n",
    "        score, labels = cls_preds.sigmoid().max(1)\n",
    "        ids = score > cls_thresh\n",
    "        ids = ids.nonzero().squeeze()\n",
    "        keep = self._box_nms(boxes[ids], score[ids], threshold=nms_thresh) # nms\n",
    "        return boxes[ids][keep], labels[ids][keep]\n",
    "\n",
    "    # cell index 생성 함수\n",
    "    def _meshgrid(self, x, y, row_major=True):\n",
    "        a = torch.arange(0,x)\n",
    "        b = torch.arange(0,y)\n",
    "        xx = a.repeat(y).view(-1,1)\n",
    "        yy = b.view(-1,1).repeat(1,x).view(-1,1)\n",
    "        return torch.cat([xx,yy],1) if row_major else torch.cat([yy,xx],1)\n",
    "    \n",
    "    # x1,y1,x2,y2 <-> cx,cy,w,h\n",
    "    def _change_box_order(self, boxes, order):\n",
    "        assert order in ['xyxy2xywh','xywh2xyxy']\n",
    "        boxes = np.array(boxes)\n",
    "        a = boxes[:,:2]\n",
    "        b = boxes[:,2:]\n",
    "        a, b = torch.Tensor(a), torch.Tensor(b)\n",
    "        if order == 'xyxy2xywh':\n",
    "            return torch.cat([(a+b)/2,b-a+1],1) # xywh\n",
    "        return torch.cat([a-b/2, a+b/2],1) # xyxy\n",
    "\n",
    "    # 두 박스의 iou 계산\n",
    "    def _box_iou(self, box1, box2, order='xyxy'):\n",
    "        if order == 'xywh':\n",
    "            box1 = self._change_box_order(box1, 'xywh2xyxy')\n",
    "            box2 = self._change_box_order(box2, 'xywh2xyxy')\n",
    "        \n",
    "        N = box1.size(0)\n",
    "        M = box2.size(0)\n",
    "\n",
    "        lt = torch.max(box1[:,None,:2], box2[:,:2])\n",
    "        rb = torch.min(box1[:,None,2:], box2[:,2:])\n",
    "\n",
    "        wh = (rb-lt+1).clamp(min=0)\n",
    "        inter = wh[:,:,0] * wh[:,:,1]\n",
    "\n",
    "        area1 = (box1[:,2]-box1[:,0]+1) * (box1[:,3]-box1[:,1]+1)\n",
    "        area2 = (box2[:,2]-box2[:,0]+1) * (box2[:,3]-box2[:,1]+1)\n",
    "        iou = inter / (area1[:,None] + area2 - inter)\n",
    "        return iou\n",
    "\n",
    "    # nms\n",
    "    def _box_nms(self, bboxes, scores, threshold=0.5, mode='union'):\n",
    "        x1 = bboxes[:,0]\n",
    "        y1 = bboxes[:,1]\n",
    "        x2 = bboxes[:,2]\n",
    "        y2 = bboxes[:,3]\n",
    "\n",
    "        areas = (x2-x1+1) * (y2-y1+1)\n",
    "        _, order = scores.sort(0, descending=True) # confidence 순 정렬\n",
    "        keep = []\n",
    "        while order.numel() > 0:\n",
    "            if order.numel() == 1:\n",
    "                keep.append(order.data)\n",
    "                break\n",
    "            i = order[0] # confidence 가장 높은 anchor 추출\n",
    "            keep.append(i) # 최종 detection에 저장\n",
    "\n",
    "            xx1 = x1[order[1:]].clamp(min=x1[i])\n",
    "            yy1 = y1[order[1:]].clamp(min=y1[i])\n",
    "            xx2 = x2[order[1:]].clamp(max=x2[i])\n",
    "            yy2 = y2[order[1:]].clamp(max=y2[i])\n",
    "\n",
    "            w = (xx2-xx1+1).clamp(min=0)\n",
    "            h = (yy2-yy1+1).clamp(min=0)\n",
    "            inter = w*h\n",
    "\n",
    "            if mode == 'union':\n",
    "                ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "            elif mode == 'min':\n",
    "                ovr = inter / areas[order[1:]].clamp(max=areas[i])\n",
    "            else:\n",
    "                raise TypeError('Unknown nms mode: %s.' % mode)\n",
    "\n",
    "            ids = (ovr<=threshold).nonzero().squeeze()\n",
    "            if ids.numel() == 0:\n",
    "                break\n",
    "            order = order[ids+1]\n",
    "        return torch.LongTensor(keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn\n",
    "# targets에 encode를 수행하고, tensor로 변경합니다.\n",
    "def collate_fn(batch):\n",
    "    encoder = DataEncoder()\n",
    "    imgs = [x[0] for x in batch]\n",
    "    boxes = [torch.Tensor(x[1]) for x in batch]\n",
    "    labels = [torch.Tensor(x[2]) for x in batch]\n",
    "    h,w = 600, 600\n",
    "    num_imgs = len(imgs)\n",
    "    inputs = torch.zeros(num_imgs, 3, h, w)\n",
    "\n",
    "    loc_targets = []\n",
    "    cls_targets = []\n",
    "    for i in range(num_imgs):\n",
    "        inputs[i] = imgs[i]\n",
    "        loc_target, cls_target = encoder.encode(boxes=boxes[i], labels=labels[i], input_size=(w,h))\n",
    "        loc_targets.append(loc_target)\n",
    "        cls_targets.append(cls_target)\n",
    "    return inputs, torch.stack(loc_targets), torch.stack(cls_targets) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_dl = DataLoader(val_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BottleNeck of ResNet\n",
    "class Bottleneck(nn.Module):\n",
    "    expand = 4\n",
    "\n",
    "    def __init__(self, in_channels, inner_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, inner_channels, 1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(inner_channels)\n",
    "        self.conv2 = nn.Conv2d(inner_channels, inner_channels, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(inner_channels)\n",
    "        self.conv3 = nn.Conv2d(inner_channels, inner_channels*self.expand, 1, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(inner_channels*self.expand)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.downsample = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != inner_channels*self.expand:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, inner_channels*self.expand, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(inner_channels*self.expand)\n",
    "            )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.relu(self.bn1(self.conv1(x)))\n",
    "        output = self.relu(self.bn2(self.conv2(output)))\n",
    "        output = self.bn3(self.conv3(output))\n",
    "        output = self.relu(output + self.downsample(x))\n",
    "\n",
    "        return output\n",
    "\n",
    "# check\n",
    "# def test():\n",
    "#     x = torch.randn(1, 56,13,13).to(device)\n",
    "#     net = Bottleneck(x.size(1), x.size(1)).to(device)\n",
    "#     output = net(x)\n",
    "#     print(output.size())\n",
    "\n",
    "# test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FPN은 ResNet의 피쳐맵에서 multi-scale로 특징을 추출합니다.\n",
    "class FPN(nn.Module):\n",
    "    def __init__(self, num_blocks):\n",
    "        super(FPN, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, 7, stride=2, padding=3, bias=False) # 300x300\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1) # 150x150\n",
    "\n",
    "        # Bottom-up layers and ResNet\n",
    "        # PyTorch 공식 홈페이지 ResNet 구현 코드와 변수명이 동일해야, pre-trained model을 불러와서 사용할 수 있습니다.\n",
    "        self.layer1 = self._make_layer(64, num_blocks[0], stride=1)  # c2, 150x150\n",
    "        self.layer2 = self._make_layer(128, num_blocks[1], stride=2)  # c3 75x75\n",
    "        self.layer3 = self._make_layer(256, num_blocks[2], stride=2) # c4 38x38\n",
    "        self.layer4 = self._make_layer(512, num_blocks[3], stride=2) # c5\n",
    "        self.conv6 = nn.Conv2d(2048, 256, 3, stride=2, padding=1)    # p6\n",
    "        self.conv7 = nn.Sequential(                                  # p7\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        # Lateral layers\n",
    "        self.lateral_1 = nn.Conv2d(2048, 256, 1, stride=1, padding=0)\n",
    "        self.lateral_2 = nn.Conv2d(1024, 256, 1, stride=1, padding=0)\n",
    "        self.lateral_3 = nn.Conv2d(512, 256, 1, stride=1, padding=0)\n",
    "\n",
    "        # Top-down layers\n",
    "        self.top_down_1 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
    "        self.top_down_2 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
    "\n",
    "        self.upsample_1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.upsample_2 = nn.Upsample(size=(75,75), mode='bilinear', align_corners=False) # size=(75,75)를 지정해야 합니다.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feature extractor(ResNet)\n",
    "        c1 = self.relu(self.bn1(self.conv1(x)))\n",
    "        c1 = self.maxpool(c1)\n",
    "        c2 = self.layer1(c1)\n",
    "        c3 = self.layer2(c2)\n",
    "        c4 = self.layer3(c3)\n",
    "        c5 = self.layer4(c4)\n",
    "\n",
    "        # FPN\n",
    "        p6 = self.conv6(c5)\n",
    "        p7 = self.conv7(p6)\n",
    "        p5 = self.lateral_1(c5)\n",
    "        p4 = self.top_down_1(self.upsample_1(p5) + self.lateral_2(c4))\n",
    "        p3 = self.top_down_2(self.upsample_2(p4) + self.lateral_3(c3))\n",
    "\n",
    "        return p3, p4, p5, p6, p7\n",
    "\n",
    "    def _make_layer(self, inner_channels, num_block, stride):\n",
    "        strides = [stride] + [1] * (num_block-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(Bottleneck(self.in_channels, inner_channels, stride=stride))\n",
    "            self.in_channels = inner_channels*Bottleneck.expand\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "def FPN50(): # ResNet-50\n",
    "    return FPN([3,4,6,3])\n",
    "\n",
    "# check\n",
    "# if __name__ == '__main__':\n",
    "#     x = torch.randn(3, 3, 600, 600).to(device)\n",
    "#     model = FPN50().to(device)\n",
    "#     outputs = model(x)\n",
    "#     for output in outputs:\n",
    "#         print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FPN 출력값을 입력으로 받아 예측을 수행합니다.\n",
    "class RetinaNet(nn.Module):\n",
    "    num_anchors = 9\n",
    "\n",
    "    def __init__(self, num_classes=20):\n",
    "        super().__init__()\n",
    "        self.fpn = FPN50()\n",
    "        self.num_classes = num_classes\n",
    "        self.loc_head = self._make_head(self.num_anchors*4) # 바운딩 박스 좌표 예측\n",
    "        self.cls_head = self._make_head(self.num_anchors*self.num_classes) # 바운딩 박스 클래스 예측\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p3: batch, channels, H, W\n",
    "        fms = self.fpn(x) # p3, p4, p5, p6, p7\n",
    "        loc_preds = []\n",
    "        cls_preds = []\n",
    "        for fm in fms: # fpn 출력값에 classifier 추가\n",
    "            loc_pred = self.loc_head(fm)\n",
    "            cls_pred = self.cls_head(fm)\n",
    "            loc_pred = loc_pred.permute(0,2,3,1).contiguous().view(x.size(0),-1,4)  # [N, 9*4,H,W] -> [N,H,W, 9*4] -> [N,H*W*9, 4]\n",
    "            cls_pred = cls_pred.permute(0,2,3,1).contiguous().view(x.size(0),-1,self.num_classes) # [N,9*20,H,W] -> [N,H,W,9*20] -> [N,H*W*9,20]\n",
    "            loc_preds.append(loc_pred)\n",
    "            cls_preds.append(cls_pred)\n",
    "        return torch.cat(loc_preds,1), torch.cat(cls_preds,1)\n",
    "\n",
    "    def _make_head(self, out_channels): # 예측을 수행하는 Layer 생성\n",
    "        layers = []\n",
    "        for _ in range(4):\n",
    "            layers.append(nn.Conv2d(256,256,3, stride=1, padding=1))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Conv2d(256, out_channels, 3, stride=1, padding=1)) # (batch,9*4,H,W) or (batch,9*20,H,W) \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def freeze_bn(self): # pre-trained model을 사용하므로, BN freeze\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.BatchNorm2d):\n",
    "                layer.eval()\n",
    "\n",
    "# check\n",
    "# if __name__ == '__main__':\n",
    "#     x = torch.randn(10,3,600,600).to(device)\n",
    "#     model = RetinaNet().to(device)\n",
    "#     loc_preds, cls_preds = model(x)\n",
    "#     print(loc_preds.size()) # (batch, 5 * H*W * 9, 4)\n",
    "#     print(cls_preds.size()) # (batch, 5 * H*W * 9, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2weight = cpath+'\\content\\models\\Resnet50-19c8e357.pth' # 가중치 저장할 경로\n",
    "d = torch.load(path2weight) # 사전학습 가중치 읽어오기\n",
    "fpn = FPN50()               # FPN50 생성\n",
    "dd = fpn.state_dict()       # fpn 가중치 파일 추출\n",
    "for k in d.keys():          # 사전학습 가중치로부터 가중치 추출\n",
    "    if not k.startswith('fc'): # fc layer 제외\n",
    "        dd[k] = d[k]        # 변수 명이 동일한 경우, 가중치 받아오기\n",
    "\n",
    "model = RetinaNet()         # RetinaNet 가중치 초기화\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.normal_(m.weight, mean=0, std=0.01)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        m.weight.data.fill_(1)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "pi = 0.01\n",
    "init.constant_(model.cls_head[-1].bias, -math.log((1-pi)/pi))\n",
    "\n",
    "model.fpn.load_state_dict(dd)  # fpn의 가중치를 사전 학습된 가중치로 변경\n",
    "torch.save(model.state_dict(), 'model.pth') # 가중치 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_embedding(labels, num_classes):\n",
    "    # labels: class labels, sized [N,]\n",
    "    # num_classes: 클래스 수 20\n",
    "    y = torch.eye(num_classes) # [20, 20]\n",
    "    np_labels = np.array(labels)\n",
    "    return y[np_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes # VOC dataset 20\n",
    "\n",
    "    # alternative focal loss\n",
    "    def focal_loss_alt(self, x, y):\n",
    "        alpha = 0.25\n",
    "\n",
    "        t = one_hot_embedding(y.data.cpu(), 1+self.num_classes)\n",
    "        t = t[:,1:] # 배경 제외\n",
    "        t = t.cuda()\n",
    "\n",
    "        xt = x*(2*t-1) # xt = x if t > 0 else -x\n",
    "        pt = (2*xt+1).sigmoid()\n",
    "\n",
    "        w = alpha*t + (1-alpha)*(1-t)\n",
    "        loss = -w*pt.log() / 2\n",
    "        return loss.sum()\n",
    "\n",
    "    def forward(self, loc_preds, loc_targets, cls_preds, cls_targets):\n",
    "        # (loc_preds, loc_targets)와 (cls_preds, cls_targets) 사이의 loss 계산\n",
    "        # loc_preds: [batch_size, #anchors, 4]\n",
    "        # loc_targets: [batch_size, #anchors, 4]\n",
    "        # cls_preds: [batch_size, #anchors, #classes]\n",
    "        # cls_targets: [batch_size, #anchors]\n",
    "\n",
    "        # loss = SmoothL1Loss(loc_preds, loc_targets) + FocalLoss(cls_preds, cls_targets)\n",
    "\n",
    "        batch_size, num_boxes = cls_targets.size()\n",
    "        pos = cls_targets > 0\n",
    "        num_pos = pos.data.long().sum()\n",
    "\n",
    "        # loc_loss = SmoothL1Loss(pos_loc_preds, pos_loc_targets)\n",
    "        mask = pos.unsqueeze(2).expand_as(loc_preds) # [N, #anchors, 4], 객체가 존재하는 앵커박스 추출\n",
    "        masked_loc_preds = loc_preds[mask].view(-1,4)      # [#pos, 4]\n",
    "        masked_loc_targets = loc_targets[mask].view(-1, 4) # [#pos, 4]\n",
    "        loc_loss = F.smooth_l1_loss(masked_loc_preds, masked_loc_targets, reduction='sum')\n",
    "\n",
    "        # cls_loss = FocalLoss(loc_preds, loc_targets)\n",
    "        pos_neg = cls_targets > -1 # ground truth가 할당되지 않은 anchor 삭제\n",
    "        mask = pos_neg.unsqueeze(2).expand_as(cls_preds)\n",
    "        masked_cls_preds = cls_preds[mask].view(-1, self.num_classes)\n",
    "        cls_loss = self.focal_loss_alt(masked_cls_preds, cls_targets[pos_neg])\n",
    "\n",
    "        # print('loc_loss: %.3f | cls_loss: %.3f' % (loc_loss.item(), cls_loss))\n",
    "        loss = (loc_loss+cls_loss)/num_pos\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = FocalLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=15)\n",
    "\n",
    "# 현재 lr 계산\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "# batch당 loss 계산\n",
    "def loss_batch(loss_func, loc_preds, loc_targets, cls_preds, cls_targets, opt=None):\n",
    "    loss_b = loss_func(loc_preds, loc_targets, cls_preds, cls_targets)\n",
    "    \n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss_b.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    return loss_b.item()\n",
    "\n",
    "# epoch당 loss 계산\n",
    "def loss_epoch(model, loss_func, dataset_dl, sanity_check=False, opt=None):\n",
    "    running_loss = 0.0\n",
    "    len_data = len(dataset_dl.dataset)\n",
    "\n",
    "    for img, loc_targets, cls_targets in dataset_dl:\n",
    "        img, loc_targets, cls_targets = img.to(device), loc_targets.to(device), cls_targets.to(device)\n",
    "        loc_preds, cls_preds = model(img)\n",
    "\n",
    "        loss_b = loss_batch(loss_func, loc_preds, loc_targets, cls_preds, cls_targets, opt)\n",
    "        \n",
    "        running_loss += loss_b\n",
    "\n",
    "        if sanity_check is True:\n",
    "            break\n",
    "\n",
    "    loss = running_loss / len_data\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 시작하는 함수\n",
    "def train_val(model, params):\n",
    "    num_epochs=params['num_epochs']\n",
    "    loss_func=params['loss_func']\n",
    "    opt=params['optimizer']\n",
    "    train_dl=params['train_dl']\n",
    "    val_dl=params['val_dl']\n",
    "    sanity_check=params['sanity_check']\n",
    "    lr_scheduler=params['lr_scheduler']\n",
    "    path2weights=params['path2weights']\n",
    "\n",
    "    loss_history = {'train': [], 'val': []}\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    torch.save(model.state_dict(),path2weights)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = get_lr(opt)\n",
    "        print('Epoch {}/{}, current lr = {}'.format(epoch, num_epochs-1, current_lr))\n",
    "\n",
    "        model.train()\n",
    "        train_loss = loss_epoch(model, loss_func, train_dl, sanity_check, opt)\n",
    "        loss_history['train'].append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = loss_epoch(model, loss_func, val_dl, sanity_check)\n",
    "        loss_history['val'].append(val_loss)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(),path2weights)\n",
    "            print('Copied best model weights!')\n",
    "\n",
    "        lr_scheduler.step(val_loss)\n",
    "\n",
    "        if current_lr != get_lr(opt):\n",
    "            print('Loading best model weights')\n",
    "            model.load_state_dict(torch.load(path2weight))\n",
    "\n",
    "        print('train loss: %.6f, val loss: %.6f, time: %.4f min' %(train_loss, val_loss, (time.time()-start_time)/60))\n",
    "\n",
    "    model.load_state_dict(torch.load(path2weight))\n",
    "    return model, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 파라미터 정의\n",
    "params_train = {\n",
    "    'num_epochs':150,\n",
    "    'optimizer':opt,\n",
    "    'loss_func':loss_func,\n",
    "    'train_dl':train_dl,\n",
    "    'val_dl':val_dl,\n",
    "    'sanity_check':False,\n",
    "    'lr_scheduler':lr_scheduler,\n",
    "    'path2weights':cpath+'\\models\\weights.pt',\n",
    "}\n",
    "\n",
    "# 가중치 저장할 폴더 생성\n",
    "import os\n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSerror:\n",
    "        print('Error')\n",
    "createFolder(cpath+'\\models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/149, current lr = 0.001\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Float (got Long)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-dd6b3acd69d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#  학습하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRetinaNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_hist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_val\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-48966890f5fe>\u001b[0m in \u001b[0;36mtrain_val\u001b[1;34m(model, params)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msanity_check\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mloss_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-b69aa74bb0c6>\u001b[0m in \u001b[0;36mloss_epoch\u001b[1;34m(model, loss_func, dataset_dl, sanity_check, opt)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mlen_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_dl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls_targets\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset_dl\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc_targets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls_targets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mloc_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\TorchCH\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\TorchCH\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-2a5b29d80e86>\u001b[0m in \u001b[0;36mcollate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_imgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mloc_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mloc_targets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mcls_targets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-c38f523b63f8>\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, boxes, labels, input_size)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0minput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0manchor_boxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_anchor_boxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 앵커 박스 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_change_box_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'xyxy2xywh'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# xyxy -> cxcywh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-c38f523b63f8>\u001b[0m in \u001b[0;36m_get_anchor_boxes\u001b[1;34m(self, input_size)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mxy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_meshgrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfm_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfm_h\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;31m#[fm_h * fm_w, 2] 피쳐맵 cell index 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;31m#print(xy.dtype)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mxy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[1;31m#xy = torch.cuda.FloatTensor(xy)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Float (got Long)"
     ]
    }
   ],
   "source": [
    "#  학습하기\n",
    "model=RetinaNet().to(device)\n",
    "model, loss_hist = train_val(model, params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = params_train['num_epochs']\n",
    "\n",
    "# Plot train-val loss\n",
    "plt.title('Train-Val Loss')\n",
    "plt.plot(range(1, num_epochs+1), loss_hist['train'], label='train')\n",
    "plt.plot(range(1, num_epochs+1), loss_hist['val'], label='val')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Training Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RetinaNet().to(device)\n",
    "model.load_state_dict(torch.load(cpath+'\\content\\models\\weights.pt'))\n",
    "model.eval()\n",
    "\n",
    "# test set trainforms 적용\n",
    "IMAGE_SIZE = 600\n",
    "scale = 1.0\n",
    "\n",
    "test_transforms = A.Compose([\n",
    "                    A.LongestMaxSize(max_size=int(IMAGE_SIZE * scale)),\n",
    "                    A.PadIfNeeded(min_height=int(IMAGE_SIZE*scale), min_width=int(IMAGE_SIZE*scale),border_mode=cv2.BORDER_CONSTANT),\n",
    "                    ToTensorV2()\n",
    "                    ])\n",
    "\n",
    "# test 이미지 불러오기\n",
    "img = Image.open(cpath+'\\content\\Voc\\VOCdevkit\\VOC2007\\JPEGImages\\000007.jpg')\n",
    "w = h = 600\n",
    "img = np.array(img.convert('RGB'))\n",
    "img = test_transforms(image=img)\n",
    "img = img['image']\n",
    "\n",
    "x = img.unsqueeze(0).to(device) # [batch, H, W, 3]\n",
    "loc_preds, cls_preds = model(x)\n",
    "\n",
    "encoder = DataEncoder()\n",
    "loc_preds, cls_preds = loc_preds.to('cpu'), cls_preds.to('cpu')\n",
    "\n",
    "# nms 수행 및 출력 값을 바운딩박스 형태로 받아오기\n",
    "boxes, labels = encoder.decode(loc_preds.data.squeeze(), cls_preds.data.squeeze(), (w,h))\n",
    "\n",
    "# 이미지 출력\n",
    "img = transforms.ToPILImage()(img)\n",
    "draw = ImageDraw.Draw(img)\n",
    "for box in boxes:\n",
    "    draw.rectangle(list(box), outline='red')\n",
    "plt.imshow(np.array(img))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
